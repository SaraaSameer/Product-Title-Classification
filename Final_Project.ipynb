{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff8c1f6",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6859c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#------ Additional Lib\n",
    "import neattext as nt\n",
    "import neattext.functions as nfx\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer   # Turning textual data into numeric for computation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder               # For encoding categorical target attr\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm   # Baseline\n",
    "\n",
    "# ------- Validation metrics\n",
    "from sklearn.metrics import accuracy_score    \n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import hamming_loss                  \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe521d8",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d17174e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Computer\n",
      "[nltk_data]     Point\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_lvl1</th>\n",
       "      <th>category_lvl2</th>\n",
       "      <th>category_lvl3</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fashion</td>\n",
       "      <td>Women</td>\n",
       "      <td>Muslim Wear</td>\n",
       "      <td>adana galleri suri squar hijab light pink ul l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "      <td>Bath &amp; Body</td>\n",
       "      <td>Hand &amp; Foot Care</td>\n",
       "      <td>cuba heartbreak eau de parfum spray ml oz form...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TV, Audio / Video, Gaming &amp; Wearables</td>\n",
       "      <td>Audio</td>\n",
       "      <td>Live Sound &amp; Stage</td>\n",
       "      <td>andoer cm cellphon smartphon mini dual head om...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "      <td>Hair Care</td>\n",
       "      <td>Shampoos &amp; Conditioners</td>\n",
       "      <td>anmyna complaint silki set shampoo ml conditio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "      <td>Men's Care</td>\n",
       "      <td>Body and Skin Care</td>\n",
       "      <td>argit argiltubo green clay face bodi ml ul li ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36278</th>\n",
       "      <td>Computers &amp; Laptops</td>\n",
       "      <td>Computer Accessories</td>\n",
       "      <td>Keyboards</td>\n",
       "      <td>sade k led backlit wire usb mechan game keyboa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36279</th>\n",
       "      <td>Home Appliances</td>\n",
       "      <td>Large Appliances</td>\n",
       "      <td>Microwaves &amp; Ovens</td>\n",
       "      <td>sona l electr oven seo ul li nbsp year warrant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36280</th>\n",
       "      <td>Computers &amp; Laptops</td>\n",
       "      <td>Computer Accessories</td>\n",
       "      <td>Speakers</td>\n",
       "      <td>op portabl wireless bluetooth speaker hand fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36281</th>\n",
       "      <td>Home &amp; Living</td>\n",
       "      <td>Bedding</td>\n",
       "      <td>Pillows &amp; Bolsters</td>\n",
       "      <td>woot woot tictacto pillow case white ul li cot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36282</th>\n",
       "      <td>TV, Audio / Video, Gaming &amp; Wearables</td>\n",
       "      <td>Wearable Technology</td>\n",
       "      <td>Activity Trackers</td>\n",
       "      <td>new smart wristband smart bracelet heart rate ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36283 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               category_lvl1         category_lvl2  \\\n",
       "0                                    Fashion                 Women   \n",
       "1                            Health & Beauty           Bath & Body   \n",
       "2      TV, Audio / Video, Gaming & Wearables                 Audio   \n",
       "3                            Health & Beauty             Hair Care   \n",
       "4                            Health & Beauty            Men's Care   \n",
       "...                                      ...                   ...   \n",
       "36278                    Computers & Laptops  Computer Accessories   \n",
       "36279                        Home Appliances      Large Appliances   \n",
       "36280                    Computers & Laptops  Computer Accessories   \n",
       "36281                          Home & Living               Bedding   \n",
       "36282  TV, Audio / Video, Gaming & Wearables   Wearable Technology   \n",
       "\n",
       "                 category_lvl3  \\\n",
       "0                  Muslim Wear   \n",
       "1             Hand & Foot Care   \n",
       "2           Live Sound & Stage   \n",
       "3      Shampoos & Conditioners   \n",
       "4           Body and Skin Care   \n",
       "...                        ...   \n",
       "36278                Keyboards   \n",
       "36279       Microwaves & Ovens   \n",
       "36280                 Speakers   \n",
       "36281       Pillows & Bolsters   \n",
       "36282        Activity Trackers   \n",
       "\n",
       "                                                combined  \n",
       "0      adana galleri suri squar hijab light pink ul l...  \n",
       "1      cuba heartbreak eau de parfum spray ml oz form...  \n",
       "2      andoer cm cellphon smartphon mini dual head om...  \n",
       "3      anmyna complaint silki set shampoo ml conditio...  \n",
       "4      argit argiltubo green clay face bodi ml ul li ...  \n",
       "...                                                  ...  \n",
       "36278  sade k led backlit wire usb mechan game keyboa...  \n",
       "36279  sona l electr oven seo ul li nbsp year warrant...  \n",
       "36280  op portabl wireless bluetooth speaker hand fre...  \n",
       "36281  woot woot tictacto pillow case white ul li cot...  \n",
       "36282  new smart wristband smart bracelet heart rate ...  \n",
       "\n",
       "[36283 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "def get_train_dataset():\n",
    "    return pd.read_csv('data_train.csv', header=None, names=[\"country\", \"sku_id\", \"title\", \"category_lvl1\",\"category_lvl2\",\n",
    "                                                       \"category_lvl3\", \"description\", \"price\", \"type\"])  \n",
    "\n",
    "def preprocess(content):\n",
    "    ps = PorterStemmer()\n",
    "    CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    # Using str(content) because there are some float values in combined\n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ',str(content))   # Dropping all encodings, numbers etc\n",
    "    stemmed_content = re.sub(CLEANR, '',stemmed_content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [ps.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content\n",
    "\n",
    "\n",
    "def feature_select(df):\n",
    "    df.drop(['country','sku_id','price','type'],inplace=True,axis=1)\n",
    "    df['combined'] = df['title']+\" \"+df['description']\n",
    "    df.drop(['title', 'description'],inplace=True,axis=1)  #this line is not tested\n",
    "    df['combined']\n",
    "    return df\n",
    "    \n",
    "    \n",
    "\n",
    "train_df = get_train_dataset()\n",
    "train_df = feature_select(train_df)\n",
    "train_df['combined'] = train_df['combined'].apply(preprocess)\n",
    "train_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a23455b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_lvl1       0\n",
      "category_lvl2       0\n",
      "category_lvl3    2135\n",
      "combined            0\n",
      "dtype: int64\n",
      "\n",
      "The missing data percent is: 1.4710746079431138\n",
      "[2 3 7 1 0 4 8 6 5]\n",
      "[56 3 1 23 36 29 7 28 27 55 33 35 54 0 16 10 24 44 21 5 19 52 8 43 47 4 46\n",
      " 11 38 25 9 15 32 14 12 49 22 45 40 50 34 20 53 26 30 31 2 13 17 48 18 6\n",
      " 41 39 51 42 37]\n",
      "[113 67 95 147 23 165 91 162 37 179 2 46 149 106 125 26 180 49 170 111 nan\n",
      " 9 59 17 143 27 24 103 31 172 99 55 78 14 58 15 50 150 64 10 124 145 152\n",
      " 134 115 93 144 43 183 73 1 135 126 177 159 41 47 153 79 97 70 118 92 139\n",
      " 39 87 7 171 94 48 131 52 11 105 120 108 83 63 71 69 128 122 133 65 34 182\n",
      " 62 82 158 100 38 130 61 167 56 32 60 19 42 53 89 174 116 129 146 12 40 29\n",
      " 0 148 28 25 21 140 160 157 132 68 18 30 85 112 161 175 96 109 90 76 164\n",
      " 119 166 3 74 141 156 80 154 136 20 137 107 142 44 86 51 75 163 181 123 54\n",
      " 5 77 104 121 35 45 173 66 36 4 155 117 138 114 13 168 22 88 16 102 57 84\n",
      " 72 151 110 169 8 6 178 33 176 127 98 101 81]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Computer Point\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_lvl1    0\n",
      "category_lvl2    0\n",
      "category_lvl3    0\n",
      "Title_desc       0\n",
      "dtype: int64\n",
      "\n",
      "The missing data percent is: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_missing_stats(df):\n",
    "    missing_val = df.isnull().sum()\n",
    "    print(missing_val)\n",
    "    total_cells = np.product(df.shape)\n",
    "    missing_percent = (missing_val.sum()/total_cells) *100\n",
    "    print(f'\\nThe missing data percent is: {missing_percent}')\n",
    "\n",
    "def preserve_label():    \n",
    "    #np.argmax for decoding final predicted result\n",
    "#     train_df['category_lvl3'].astype(str)\n",
    "#     train_df['category_lvl3'] = train_df['category_lvl3'].values\n",
    "    labels_c1=train_df['category_lvl1'].unique()\n",
    "    labels_c2=train_df['category_lvl2'].unique()\n",
    "    labels_c3=train_df['category_lvl3'].unique()\n",
    "    return labels_c1, labels_c2, labels_c3\n",
    "    \n",
    "\n",
    "def encode_utility(data):\n",
    "    encoder = LabelEncoder()\n",
    "    '''function to encode non-null data and replace it in the original data'''\n",
    "    #retains only non-null values\n",
    "    nonulls = np.array(data.dropna())\n",
    "    #reshapes the data for encoding\n",
    "    impute_reshape = nonulls.reshape(-1,1)\n",
    "    #encode date\n",
    "    impute_ordinal = encoder.fit_transform(impute_reshape)\n",
    "#     impute_ordinal = pd.get_dummies(impute_reshape).values # one hot encoding   #Not working\n",
    "    #Assign back encoded values to non-null values\n",
    "    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n",
    "    return data\n",
    "\n",
    "def encode(target):\n",
    "    for columns in target:\n",
    "        encode_utility(train_df[columns])\n",
    "\n",
    "    \n",
    "def impute():\n",
    "    imputer = KNNImputer(n_neighbors = 5)\n",
    "    df_imputed = np.round(imputer.fit_transform(train_df[['category_lvl1', 'category_lvl2', 'category_lvl3']]))\n",
    "    return df_imputed\n",
    "\n",
    "def clean_csv(df):\n",
    "    df = pd.DataFrame(df, columns = ['category_lvl1','category_lvl2','category_lvl3'])\n",
    "    df ['Title_desc'] = train_df['combined']\n",
    "    df.to_csv('train_clean.csv')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "print_missing_stats(train_df)\n",
    "label_c1, label_c2, label_c3 = preserve_label()\n",
    "print(label_c1)\n",
    "print(label_c2)\n",
    "print(label_c3)\n",
    "encode(['category_lvl1', 'category_lvl2', 'category_lvl3'])\n",
    "df_imputed = impute()\n",
    "df_imputed = clean_csv(df_imputed)\n",
    "print_missing_stats(df_imputed)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afbfbb43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_imputed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n\u001b[1;32m---> 19\u001b[0m X, Y1, Y2, Y3 \u001b[38;5;241m=\u001b[39m extract_features(\u001b[43mdf_imputed\u001b[49m)\n\u001b[0;32m     20\u001b[0m X \u001b[38;5;241m=\u001b[39m tf_idf(X)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_imputed' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_features(df):\n",
    "    # Extract features prior to encoding to retain categorical data\n",
    "    X = df['Title_desc']\n",
    "    Y1 = df['category_lvl1']\n",
    "    Y2 = df['category_lvl2']\n",
    "    Y3 = df['category_lvl3']\n",
    "    return X,Y1,Y2,Y3\n",
    "\n",
    "\n",
    "def tf_idf(X):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(X)\n",
    "    X = vectorizer.transform(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X, Y1, Y2, Y3 = extract_features(df_imputed)\n",
    "X = tf_idf(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb90b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
